# RAG Evaluation (LLM as a Judge)

This repository contains a Jupyter notebook for evaluating Retrieval-Augmented Generation (RAG) models using Large Language Models (LLMs) as judges.

## ðŸ“Œ Overview

The notebook provides an automated approach to assess the quality of responses generated by RAG models. It leverages an LLM to evaluate outputs based on specific criteria.

## ðŸš€ Installation

### 1. Clone the Repository

```sh
git clone https://<your-project-git-folder>
cd <your-project-folder>
```

### 2. Install Dependencies using Poetry

Ensure you have Poetry installed, then run:

```sh
poetry install
```

## ðŸ›  Usage

### Run the Jupyter Notebook

Start the notebook server:

```sh
poetry run jupyter notebook
```

Open the `RAG_Evaluation_LLM_as_a_Judge.ipynb` file and execute the cells.

## ðŸ“‚ File Structure

```
.
â”œâ”€â”€ RAG_Evaluation_LLM_as_a_Judge.ipynb  # Main notebook for evaluation
â”œâ”€â”€ pyproject.toml                    # Poetry dependency manager file
â”œâ”€â”€ README.md                             # Documentation
```

## ðŸ“Š Notebook Contents


### 1. Generation of Evaluation Dataset (Pairs of Questions and Answers)

	a. Reading the knowledge base (PDF files in our case)

	b. Using a generator LLM to generate question-answer pairs from the given document context

	c. Performing quality control with a judge LLM (different from the generator LLM), which scores each question-answer-context sample to filter out bad samples
	
### 2. Evaluation of RAG Performance Using the Generated Dataset (LLM as a Judge)

	a. Connecting to the (remote) RAG system (FlowiseAI in this case)

	b. Running the test by asking the RAG chatbot the generated questions and recording the responses

	c. Defining the evaluation prompt and evaluation function

	d. Scoring the generated answers

	e. Normalizing the evaluation scores

	f. Averaging the scores over all the questions

	g. Comparing performance via visualization
