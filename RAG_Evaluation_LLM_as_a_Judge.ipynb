{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation (LLM as a Judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation-Dataset Generation Workflow\n",
    "\n",
    "The basic workflow for automatically generating a RAG dataset starts with reading our knowledge base from documents, such as PDF files.\n",
    "\n",
    "Then we ask a generator LLM to generate question-answer pairs from the given document context.\n",
    "\n",
    "Finally, we use a judge LLM to perform quality control. The LLM will give each question-answer-context sample a score, which we can use to filter out bad samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by: The OpenAI cookbook “RAG Evaluation”\n",
    "(Link: https://huggingface.co/learn/cookbook/rag_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install from PyPI\n",
    "#%pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from openai import \n",
    "from openai import AzureOpenAI\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "  azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "# simple test\n",
    "chat_completion = openai_client.chat.completions.create(\n",
    "    model=os.getenv(\"AZURE_DEPLOYMENT_ID\"), # in my case: \"models-gpt-4o\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Does Azure OpenAI support customer managed keys?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yes, customer managed keys are supported by Azure OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Do other Azure AI services support this too?\"}\n",
    "    ]\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Possible alternative?\n",
    "\n",
    "# #!pip install azure-ai-inference\n",
    "\n",
    "# ### Azure Inference Client\n",
    "# from azure.ai.inference import ChatCompletionsClient\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# # For Azure OpenAI endpoint\n",
    "# client = ChatCompletionsClient(\n",
    "#     endpoint=endpoint,  # Of the form https://<your-resouce-name>.openai.azure.com/openai/deployments/<your-deployment-name>\n",
    "#     credential=AzureKeyCredential(key),\n",
    "#     api_version=\"2024-06-01\",  # Azure OpenAI api-version. See https://aka.ms/azsdk/azure-ai-inference/azure-openai-api-versions\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Huggingface as alternative? (Free but very limited capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# # Use your Hugging Face API token\n",
    "# HF_TOKEN = \"your HuggingFace token here!\"\n",
    "\n",
    "# # Initialize the LLM client with authentication\n",
    "# client = HuggingFaceEndpoint(\n",
    "#     repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     huggingfacehub_api_token=HF_TOKEN\n",
    "# )\n",
    "\n",
    "# # Let’s perform a quick sanity check to see that everything works as expected:\n",
    "\n",
    "# response = client.invoke(\"Say this is a test\")\n",
    "# print(response)  # Directly prints the response string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK\n",
    "The Natural Language Toolkit: Is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "project_path = \"<your-project-path\"\n",
    "nltk_path = os.path.join(project_path, \"nltk_data\")\n",
    "\n",
    "if os.path.exists(nltk_path):\n",
    "    print(\"Punkt tokenizer directory exists. Files inside:\")\n",
    "    print(os.listdir(nltk_path))\n",
    "else:\n",
    "    print(\"Punkt tokenizer is missing.\")\n",
    "\n",
    "nltk.data.path.append(nltk_path)\n",
    "#nltk.download('averaged_perceptron_tagger', download_dir=nltk_path)\n",
    "#nltk.download('punkt', download_dir=nltk_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check current directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the NLTK package is recognized\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK 'punkt' tokenizer is available.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK 'punkt' tokenizer is missing. Please check your installation.\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    print(\"NLTK 'averaged_perceptron_tagger' tokenizer is available.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK 'averaged_perceptron_tagger' not found. Please download it manually using nltk.download('averaged_perceptron_tagger')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LangChain to read a folder with all our files.\n",
    "\n",
    "First, we need to install the necessary packages. LangChain’s DirectoryLoader uses the unstructured library to read all kinds of file types. In this notebook, I will only be reading PDFs so we can install a smaller version of unstructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-community unstructured[pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read our data folder to get the LangChain documents. The following code first loads all the PDF files from a folder and then chunks them into relatively large chunks of size 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader  # type: ignore\n",
    "documents_path = os.path.join(project_path, r\"flowiseai\\eu_ai_act\\document_store\\EN\")\n",
    "loader = DirectoryLoader(documents_path, glob=\"**/*.pdf\", show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Splitting/Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in docs:\n",
    "    docs_processed.extend(text_splitter.split_documents([doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify chunked documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML \n",
    "\n",
    "formatted_docs = \"\\n\\n\".join([\n",
    "    f\"<div style='margin-bottom: 20px; padding: 10px; border-bottom: 1px solid #ddd;'>\"\n",
    "    f\"<h3>Document {i+1}</h3><pre>{doc}</pre></div>\"\n",
    "    for i, doc in enumerate(docs_processed)\n",
    "])\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "    <div style=\"max-height: 400px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;\">\n",
    "        {formatted_docs}\n",
    "    </div>\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list 'docs_processed' with items of the type Document. Each document has some metadata and the actual page_content.\n",
    "\n",
    "This list of documents is our knowledge base from which we will create question-answer pairs based on the context of the page_content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Question-Answer-Context Samples\n",
    "Using the OpenAI client and the model we created earlier, we first write a generator function to create questions and answers from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "def qa_generator_llm(context: str, client, model: str = os.getenv(\"AZURE_DEPLOYMENT_ID\")): # original model: AMead10/Llama-3.2-3B-Instruct-AWQ\n",
    "    generation_prompt = f\"\"\"\n",
    "    Your task is to write a factoid question and an answer given a context.\n",
    "    Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "    Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "    This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Output:::\n",
    "    Factoid question: (your factoid question)\n",
    "    Answer: (your answer to the factoid question)\n",
    "\n",
    "    Now here is the context.\n",
    "\n",
    "    Context: {context}\\n\n",
    "    Output:::\n",
    "    \"\"\"\n",
    "\n",
    "    # Send request to the model\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": generation_prompt}],\n",
    "        temperature=0.5,\n",
    "        max_tokens=500,\n",
    "        top_p=0.99\n",
    "    )\n",
    "    \n",
    "    return response  # The response should contain the generated question-answer pair\n",
    "\n",
    "# Example usage:\n",
    "client = openai_client\n",
    "context = \"Albert Einstein developed the theory of relativity, which revolutionized modern physics.\"\n",
    "output = qa_generator_llm(context, client)\n",
    "\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qa_generator_llm(context: str, client: OpenAI, model: str = \"AMead10/Llama-3.2-3B-Instruct-AWQ\"):\n",
    "#     generation_prompt = \"\"\"\n",
    "# Your task is to write a factoid question and an answer given a context.\n",
    "# Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "# Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "# This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Output:::\n",
    "# Factoid question: (your factoid question)\n",
    "# Answer: (your answer to the factoid question)\n",
    "\n",
    "# Now here is the context.\n",
    "\n",
    "# Context: {context}\\n\n",
    "# Output:::\"\"\"\n",
    "\n",
    "#     chat_completion = client.chat.completions.create(\n",
    "#         messages=[\n",
    "#             {\n",
    "#                 \"role\": \"system\",\n",
    "#                 \"content\": \"You are a question-answer pair generator.\"\n",
    "#             },\n",
    "#             {\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": generation_prompt.format(context=context),\n",
    "#             }\n",
    "#         ],\n",
    "#         model=model,\n",
    "#         temperature=0.5,\n",
    "#         top_p=0.99,\n",
    "#         max_tokens=500\n",
    "#     )\n",
    "\n",
    "#     return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a language other than English, you will need to translate the generation_prompt (and the system instruction).\n",
    "\n",
    "Next, we simply loop through all of our document chunks in our knowledge base and generate a question and an answer for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of document chunks\n",
    "count = len(docs_processed)\n",
    "print(\"Number of document chunks: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # Instantly make your loops show a progress meter\n",
    "\n",
    "outputs = []\n",
    "num_questions = 100  # Change this to the desired number of Q&A pairs\n",
    "print(f\"Generating {num_questions} QA couples...\")\n",
    "\n",
    "for doc in tqdm(docs_processed[:num_questions]):\n",
    "#for doc in tqdm(docs_processed): # in this case the number of generated qa is = size(docs_processed)\n",
    "    \n",
    "    # Generate QA couple\n",
    "    output_QA = qa_generator_llm(doc.page_content, client).choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        question = output_QA.split(\"Factoid question: \")[-1].split(\"Answer: \")[0].strip()\n",
    "        answer = output_QA.split(\"Answer: \")[-1].strip()\n",
    "        assert len(answer) < 500, \"Answer is too long\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": doc.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": doc.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how many PDF files are used, this may take a while... <br> \n",
    "Don’t forget to translate the strings in output_QA.split if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The genrated question-answers are in \", str(type(outputs)), \" format\")\n",
    "display(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the QA to JSON Format\n",
    "To generate a RAG evaluation dataset, I used a PDF about the regulation of the EU AI Act from the European Union. <br>\n",
    "Here is my generated raw outputs dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "json_output = json.dumps(outputs, indent=4, ensure_ascii=False)\n",
    "display(HTML(f'<div style=\"white-space: pre-wrap; overflow-y: auto; height: 300px; border: 1px solid #ccc;\">{json_output}</div>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out Bad Question-Answer Pairs\n",
    "Next, we use an LLM as a judge to automatically filter out bad samples.\n",
    "\n",
    "When using an LLM as a judge to evaluate the quality of a sample, it is best practice to use a different model than the one that was used to generate it because of a self-preference bias.\n",
    "\n",
    "When it comes to judging our generated questions and answers, there are a lot of possible prompts we could use.\n",
    "\n",
    "To build our prompt, there is a structure we can use from the G-Eval paper:\n",
    "\n",
    "We start with the task introduction\n",
    "We present our evaluation criteria\n",
    "We want the model to perform chain-of-thought (CoT) reasoning to improve its performance\n",
    "We ask for the total score at the end\n",
    "For the evaluation criteria, we can use a list where each criterion adds one point if it is fulfilled.\n",
    "\n",
    "The evaluation criteria should ensure that the question, the answer, and the context all fit together and make sense.\n",
    "\n",
    "Here are two evaluation criteria from the OpenAI RAG evaluation cookbook:\n",
    "\n",
    "**Groundedness**: can the question be answered from the given context?<br>\n",
    "**Stand-alone**: is the question understandable without any context? (To avoid a question like \"What is the name of the function used in this guide?\")\n",
    "\n",
    "And two more evaluation criteria from the RAGAS paper:\n",
    "\n",
    "**Faithfulness**: the answer should be grounded in the given context<br>\n",
    "**Answer Relevance**: the answer should address the actual question posed\n",
    "\n",
    "You can try to add more criteria or change the text for the ones that I used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the judge_llm() function, which critiques a question, answer, and context sample and produces a total rating score at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_llm(\n",
    "    context: str,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    client,\n",
    "    eval_model: str, # this models needs to be different from the model which generated the qa\n",
    "):\n",
    "    critique_prompt = \"\"\"\n",
    "    You will be given a question, answer, and a context.\n",
    "    Your task is to provide a total rating using the additive point scoring system described below.\n",
    "    Points start at 0 and are accumulated based on the satisfaction of each evaluation criterion:\n",
    "\n",
    "    Evaluation Criteria:\n",
    "    - Groundedness: Can the question be answered unambiguously from the given context? Add 1 point if the question can be answered from the context\n",
    "    - Stand-alone: Is the question understandable free of any context, for someone with domain knowledge/Internet access? Add 1 point if the question is independent and can stand alone.\n",
    "    - Faithfulness: The answer should be grounded in the given context. Add 1 point if the answer can be derived from the context\n",
    "    - Answer Relevance: The generated answer should address the actual question that was provided. Add 1 point if the answer actually answers the question\n",
    "\n",
    "    Provide your answer as follows:\n",
    "\n",
    "    Answer:::\n",
    "    Evaluation: (your rationale for the rating, as a text)\n",
    "    Total rating: (your rating, as an integer number between 0 and 4)\n",
    "\n",
    "    You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "    Now here are the question, answer, and context.\n",
    "\n",
    "    Question: {question}\\n\n",
    "    Answer: {answer}\\n\n",
    "    Context: {context}\\n\n",
    "    Answer::: \"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a neutral judge.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": critique_prompt.format(\n",
    "                    question=question, answer=answer, context=context\n",
    "                ),\n",
    "            },\n",
    "        ],\n",
    "        model=eval_model,\n",
    "        temperature=0.1,\n",
    "        top_p=0.99,\n",
    "        max_tokens=800\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through our generated dataset and critique each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook compatibility\n",
    "\n",
    "qa_evaluator_client = openai_client\n",
    "evaluation_progress_bar = tqdm(total=len(outputs), desc=\"Evaluating Outputs\", unit=\"evaluation\")\n",
    "for output in outputs:\n",
    "    try:\n",
    "        evaluation = judge_llm(\n",
    "            context=output[\"context\"],\n",
    "            question=output[\"question\"],\n",
    "            answer=output[\"answer\"],\n",
    "            client=qa_evaluator_client,\n",
    "            eval_model= \"models-gpt-35-turbo\",\n",
    "        )\n",
    "        score, eval = (\n",
    "            #int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
    "            math.floor(float(evaluation.split(\"Total rating: \")[-1].strip())),\n",
    "            evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
    "        )\n",
    "        output.update(\n",
    "            {\n",
    "                \"score\": score,\n",
    "                \"eval\": eval\n",
    "            }\n",
    "        )\n",
    "        print(\"evaluation score of: \", str(score) ,\" \\n for question: \", output[\"question\"], \" \\n and answer: \", output[\"answer\"])  \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    evaluation_progress_bar.update(1)\n",
    "evaluation_progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s filter out all the bad samples.\n",
    "\n",
    "Since the generated dataset will be the ground truth for evaluation purposes, we should only allow very high-quality data samples. That’s why I decided to keep only samples with the highest possible score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify if every QA-pair has a valid score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in outputs:\n",
    "    item.setdefault(\"score\", 1)\n",
    "    item.setdefault(\"eval\", \"The question was not scored by the LLM. A defualt score of 1 is set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Filtering based on the score\n",
    "qa_filtered = [doc for doc in outputs if doc[\"score\"] >= 4]\n",
    "\n",
    "# Converting qa to pandas Dataframe\n",
    "qa_df = pd.DataFrame(qa_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying the number of qualified question & answers:\n",
    "count = len(qa_df)\n",
    "print(f\"{count} QA couples have passed the quality control.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is our final RAG evaluation dataset as a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First visualization option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas' display options\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "display(qa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second visualization option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set Pandas' display options\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "\n",
    "# Function to wrap long text (ensuring readability)\n",
    "def wrap_text(text, width=50):\n",
    "    if isinstance(text, str):\n",
    "        return \"<br>\".join(textwrap.wrap(text, width))  # Wraps text using <br> for HTML\n",
    "    return text\n",
    "\n",
    "# Apply text wrapping\n",
    "df2 = qa_df.map(lambda x: wrap_text(x, width=50))\n",
    "\n",
    "# Convert DataFrame to HTML\n",
    "html = df2.to_html(escape=False)\n",
    "\n",
    "\n",
    "styled_html = f\"\"\"\n",
    "    <div style=\"overflow-x: auto; overflow-y: auto; max-height: 400px; border: 1px solid #ddd; padding: 5px;\">\n",
    "        <style>\n",
    "            table {{\n",
    "                border-collapse: collapse; \n",
    "                width: 100%; \n",
    "                table-layout: auto;\n",
    "            }}\n",
    "            th, td {{\n",
    "                min-width: 30px; \n",
    "                padding: 5px; \n",
    "                word-wrap: break-word; \n",
    "                text-align: left !important;  /* Ensure left alignment for both headers and data */\n",
    "            }}\n",
    "            th {{\n",
    "                white-space: nowrap;         /* Prevent text from wrapping in headers */\n",
    "            }}\n",
    "        </style>\n",
    "        {html}\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the properly formatted DataFrame\n",
    "display(HTML(styled_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving The Dataset\n",
    "We can convert our Pandas DataFrame into a HuggingFace dataset. Then, we can save it to disk and load it later when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to dataset format from HuggingFace\n",
    "qa_dataset = Dataset.from_pandas(qa_df, split=\"test\")\n",
    "print(\"Data type after conversion: \", str(type(qa_dataset)))\n",
    "\n",
    "# save QA in HuggingFace and JSON Format:\n",
    "qa_list_path = os.path.join(project_path, r\"flowiseai\\eu_ai_act\\documents_qa\")\n",
    "qa_dataset.save_to_disk(qa_list_path)\n",
    "\n",
    "# Keys to keep\n",
    "keys_to_keep = {\"question\",\"answer\",\"source_doc\"}\n",
    "\n",
    "# Extract a subset of keys\n",
    "qa_selected = [{key: item[key] for key in keys_to_keep if key in item} for item in qa_filtered]\n",
    "\n",
    "# Convert to JSON string\n",
    "qa_json = json.dumps(qa_selected, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Writing to qa_eu_ai_act.json\n",
    "with open((qa_list_path + \"\\\\qa_eu_ai_act.json\"), \"w\") as outfile:\n",
    "    outfile.write(qa_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the (previously) saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datasets import load_dataset\n",
    "qa_dataset = qa_dataset.load_from_disk(qa_list_path)\n",
    "\n",
    "# display using pandas\n",
    "qa_df = pd.DataFrame(qa_dataset)\n",
    "display(qa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the RAG using Flowise API\n",
    "Now we have created a RAG evaluation dataset from a collection of documents.\n",
    "\n",
    "To change the domain of our RAG evaluation dataset, we simply exchange the documents that we feed to the DirectoryLoader. The documents do not have to be PDF files, they can be CSV files, markdown files, etc.\n",
    "\n",
    "To change the language of our RAG evaluation dataset, we simply translate the LLM prompts from English to another language.\n",
    "\n",
    "The next step is to connect to the RAG in the Flowise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "API_URL = \"<your-api-url>\" # QnA V 0.4\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, json=payload, verify=False)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = query({\n",
    "    \"question\": \"What is the purpose of the Regulation regarding artificial intelligence systems in the Union?\",\n",
    "})\n",
    "\n",
    "# list of available keys:\n",
    "print(\"List of available keys:\")\n",
    "print(output.keys())\n",
    "\n",
    "# Keys to extract\n",
    "keys_to_extract = ['text', 'chatId', 'sessionId','sourceDocuments']\n",
    "\n",
    "# Create a new dictionary with only the desired keys\n",
    "filtered_output = {key: output[key] for key in keys_to_extract if key in output}\n",
    "print(filtered_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the RAG system\n",
    "\n",
    "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system’s output on this evaluation dataset.\n",
    "\n",
    "To this end, we setup a judge agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the different RAG evaluation metrics, we choose to focus only on **faithfulness** since it the best end-to-end metric of our system’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from typing import Optional    \n",
    "    \n",
    "def query_rag_system(payload: dict):\n",
    "    \"\"\"Queries the remote RAG system via API and retrieves an answer.\"\"\"\n",
    "    \n",
    "    response = requests.post(API_URL, json=payload, verify=False)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if not data.get(\"text\"):  # Raise an error if the answer is empty\n",
    "            raise ValueError(f\"Error: No response received from the RAG system. Response: {data}\")\n",
    "        return data\n",
    "    else:\n",
    "        raise RuntimeError(f\"Error {response.status_code}: Failed to retrieve response. Info: {response.text}\")\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset: datasets.Dataset,\n",
    "    output_file: str,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[dict] = None,  # To pass the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    try: # load previous generations if they exist\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in eval_dataset:\n",
    "        question = example[\"question\"]\n",
    "        \n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        query_dict = {\n",
    "            \"question\": question,\n",
    "            \"overrideConfig\": {\n",
    "                \"JinaRerankRetriever_0\":{\n",
    "                    \"TOP N\": test_settings[\"rerank_topn\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    " \n",
    "        response = query_rag_system(query_dict) \n",
    "        \n",
    "        # Keys to extract\n",
    "        \n",
    "        # List of available keys (for debugging):\n",
    "        # print(\"List of available keys in the API response:\")\n",
    "        # print(response.keys())\n",
    "        \n",
    "        answer = response.get(\"text\")\n",
    "        relevant_docs = response.get(\"sourceDocuments\")\n",
    "\n",
    "        if verbose:\n",
    "            print(\"======================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f'True answer: {example[\"answer\"]}')\n",
    "\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example.get(\"source_doc\", \"Unknown\"),\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": relevant_docs,\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = str(test_settings)\n",
    "        outputs.append(result)\n",
    "\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, indent=4)\n",
    "    print(\"======================================================\")\n",
    "    print(\"Testing complete. Results saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the evaluation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "\n",
    "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
    "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_eval_model = \"models-gpt-4o-mini\"\n",
    "client = openai_client\n",
    "\n",
    "def evaluate_answers(\n",
    "    answer_path: str,\n",
    "    client,\n",
    "    evaluator_model: str,\n",
    "    evaluation_prompt_template: ChatPromptTemplate,\n",
    ") -> None:\n",
    "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
    "    answers = []\n",
    "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
    "        answers = json.load(open(answer_path, \"r\"))\n",
    "\n",
    "    for experiment in answers:\n",
    "        if f\"eval_score_{evaluator_model}\" in experiment:\n",
    "            continue # Skip already evaluated experiments\n",
    "\n",
    "        # Here is the original prompt for OpenAI:\n",
    "        # eval_prompt = evaluation_prompt_template.format_messages(\n",
    "        #     instruction=experiment[\"question\"],\n",
    "        #     response=experiment[\"generated_answer\"],\n",
    "        #     reference_answer=experiment[\"true_answer\"],\n",
    "        # )\n",
    "        \n",
    "        # Adopted prompt for Azure OpenAI:\n",
    "        eval_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a fair evaluator language model. First, provide reasoning, then give the final score from 1 to 5.\"},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt_template.format(\n",
    "            instruction=experiment[\"question\"],\n",
    "            response=experiment[\"generated_answer\"],\n",
    "            reference_answer=experiment[\"true_answer\"],\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        # Call the evaluation model\n",
    "        eval_result =client.chat.completions.create(\n",
    "            model=evaluator_model,  # Adjust this based on your service API\n",
    "            messages=eval_prompt,  # Ensure eval_prompt is a list of messages\n",
    "        )\n",
    "\n",
    "        # feedback, score = [item.strip() for item in eval_result.content.split(\"[RESULT]\")] # original command for Open AI\n",
    "        \n",
    "        # Extract response content properly\n",
    "        response_text = eval_result.choices[0].message.content  # Correct way to access model output\n",
    "\n",
    "        # Ensure result format is correct before splitting\n",
    "        if \"[RESULT]\" in response_text:\n",
    "            feedback, score = [item.strip() for item in response_text.split(\"[RESULT]\")]\n",
    "            \n",
    "        else:\n",
    "            feedback= response_text # Handle missing \"[RESULT]\"\n",
    "            print(\"A missing score is replaced with a default score value of 3\")\n",
    "            print(\"Here is the corresponding feedback:\", response_text)\n",
    "            score = 3 # Handle missing score\n",
    "            #raise ValueError(f\"Error: Missing '[RESULT]' in response: {response_text}\")\n",
    "            \n",
    "            \n",
    "        experiment[f\"eval_score_{evaluator_model}\"] = score\n",
    "        experiment[f\"eval_feedback_{evaluator_model}\"] = feedback\n",
    "\n",
    "        with open(answer_path, \"w\") as f:\n",
    "            json.dump(answers, f)\n",
    "    \n",
    "    print(\"Evaluation complete. Scores saved to\", answer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"current directory: \", cwd)\n",
    "os.chdir(qa_list_path)\n",
    "cwd = os.getcwd()\n",
    "print(\"is moved to: \", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress the warning during the development phase:\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress the InsecureRequestWarning\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run the tests and evaluate answers:<br>\n",
    "You can set the chunk size, the embedding models, and the reranking option for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./eval_output\"):\n",
    "    os.mkdir(\"./eval_output\")\n",
    "eval_dataset = qa_dataset\n",
    "\n",
    "# Define test configurations\n",
    "chunk_sizes = [200]\n",
    "embeddings_list = [\"text-embedding-ada-002\"]\n",
    "# rerank_options = [False] # If the re-ranking is implemented in the RAG-system, this option can be activated.\n",
    "rerank_topn = [4, 6]\n",
    "total_iterations = len(chunk_sizes) * len(embeddings_list) * len(rerank_topn)\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Processing Configurations\", unit=\"iteration\")\n",
    "\n",
    "for chunk_size in chunk_sizes:  # Add other chunk sizes as needed\n",
    "    \n",
    "    for embeddings in embeddings_list:  # Add other embeddings as needed\n",
    "        \n",
    "        for rerank in rerank_topn:\n",
    "            \n",
    "            # name the output file and the setting\n",
    "            settings_name = f\"chunk_{chunk_size}_embeddings_{embeddings}_rerank-top-n_{rerank}\"\n",
    "            #settings_name = f\"chunk_{chunk_size}_embeddings_{embeddings.replace('/', '~')}_rerank_{rerank}_reader-model_AzureOpenAI\"\n",
    "            output_file_name = f\"./eval_output/rag_{settings_name}.json\"\n",
    "            \n",
    "            # set the parameters which are to be optimized\n",
    "            settings = {\"rerank_topn\": rerank}\n",
    "            \n",
    "            print(f\"\\n \\n Running RAG test & evaluation for {settings_name}:\")\n",
    "            \n",
    "            # run rag test\n",
    "            print(\"\\n 1- Running test...\")\n",
    "            run_rag_tests(\n",
    "                eval_dataset=eval_dataset,\n",
    "                output_file=output_file_name,\n",
    "                verbose=True,\n",
    "                test_settings=settings,\n",
    "            )\n",
    "            \n",
    "            # run rag evaluation\n",
    "            print(\"\\n 2- Running evaluation...\")\n",
    "            evaluate_answers(\n",
    "                output_file_name,\n",
    "                openai_client,\n",
    "                client_eval_model,\n",
    "                evaluation_prompt_template,\n",
    "            )\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "            \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "outputs = []\n",
    "for file in glob.glob(\"./eval_output/*.json\"):\n",
    "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
    "    output[\"settings\"] = file\n",
    "    outputs.append(output)\n",
    "result = pd.concat(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string values to integers:\n",
    "\n",
    "# If a value in the column is a string, it attempts to convert it to an integer.\n",
    "# If it's not a string, it assigns a default value of 1.\n",
    "\n",
    "eval_score_name = \"eval_score_\" + str(client_eval_model) \n",
    "result[eval_score_name] = result[eval_score_name].apply(lambda x: int(x) if isinstance(x, str) else 1)\n",
    "\n",
    "# Normalize the evaluation scores between [0, 1]\n",
    "result[eval_score_name] = (result[eval_score_name] - 1) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging the score over the QAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"test_settings\" and calculate the mean of \"eval_score_gpt-4o-mini\"\n",
    "avg_scores_df = result.groupby(\"test_settings\", as_index=False)[eval_score_name].mean()\n",
    "\n",
    "# Rename the columns\n",
    "avg_scores_df.columns = [\"test_settings\", \"average_scores\"]\n",
    "\n",
    "# Display the new dataframe\n",
    "print(\"Average scores for the genrated dataset for different configurations: \")\n",
    "print(avg_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance comparison via visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "scores = avg_scores_df  # Make sure this is the correct DataFrame\n",
    "\n",
    "fig = px.bar(\n",
    "    scores,\n",
    "    x=\"test_settings\",  # X-axis: test settings\n",
    "    y=\"average_scores\",  # Y-axis: average scores (corrected column name)\n",
    "    color=\"average_scores\",  # Use the correct column for coloring\n",
    "    labels={\n",
    "        \"average_scores\": \"Faithfulness\",\n",
    "        \"test_settings\": \"Configuration\",\n",
    "    },\n",
    "    color_continuous_scale=\"bluered\",\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    yaxis_range=[0, 1],\n",
    "    title=\"<b>Faithfulness of different RAG configurations (normalized)</b>\",\n",
    "    xaxis_title=\"RAG settings\",\n",
    "    font=dict(size=15),\n",
    ")\n",
    "\n",
    "fig.update_coloraxes(showscale=False)\n",
    "fig.update_traces(texttemplate=\"%{y:.2f}\", textposition=\"outside\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert this notebook to python code (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse to the directory where you want to save the file.\n",
    "os.chdir(project_path + \"\\\\scripts\\\\bitbucket_repository\\\\rag-evaluation\\\\\")\n",
    "cwd = os.getcwd()\n",
    "print(\"current directory: \", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script RAG_Evaluation_LLM_as_a_Judge.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
